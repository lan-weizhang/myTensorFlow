{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "def load_data(dataset_path):\n",
    "    img = Image.open(dataset_path)\n",
    "    # 定义一个20 × 20的训练样本，一共有40个人，每个人都10张样本照片\n",
    "    img_ndarray = np.asarray(img, dtype='float64') / 256\n",
    "\n",
    "    # 记录脸数据矩阵，57 * 47为每张脸的像素矩阵\n",
    "    faces = np.empty((400, 57 * 47))\n",
    "\n",
    "    for row in range(20):\n",
    "        for column in range(20):\n",
    "            faces[20 * row + column] = np.ndarray.flatten(\n",
    "                img_ndarray[row * 57: (row + 1) * 57, column * 47 : (column + 1) * 47]\n",
    "            )\n",
    "\n",
    "    label = np.zeros((400, 40))\n",
    "    for i in range(40):\n",
    "        label[i * 10: (i + 1) * 10, i] = 1\n",
    "\n",
    "    # 将数据分成训练集，验证集，测试集\n",
    "    train_data = np.empty((320, 57 * 47))\n",
    "    train_label = np.zeros((320, 40))\n",
    "    vaild_data = np.empty((40, 57 * 47))\n",
    "    vaild_label = np.zeros((40, 40))\n",
    "    test_data = np.empty((40, 57 * 47))\n",
    "    test_label = np.zeros((40, 40))\n",
    "\n",
    "    for i in range(40):\n",
    "        train_data[i * 8: i * 8 + 8] = faces[i * 10: i * 10 + 8]\n",
    "        train_label[i * 8: i * 8 + 8] = label[i * 10: i * 10 + 8]\n",
    "\n",
    "        vaild_data[i] = faces[i * 10 + 8]\n",
    "        vaild_label[i] = label[i * 10 + 8]\n",
    "\n",
    "        test_data[i] = faces[i * 10 + 9]\n",
    "        test_label[i] = label[i * 10 + 9]\n",
    "\n",
    "    return [\n",
    "        (train_data, train_label),\n",
    "        (vaild_data, vaild_label),\n",
    "        (test_data, test_label)\n",
    "    ]\n",
    "\n",
    "def convolutional_layer(data, kernel_size, bias_size, pooling_size):\n",
    "    kernel = tf.get_variable(\"conv\", kernel_size, initializer=tf.random_normal_initializer())\n",
    "    tf.summary.histogram('conv_kernel_histogram', kernel)\n",
    "    bias = tf.get_variable('bias', bias_size, initializer=tf.random_normal_initializer())\n",
    "    tf.summary.histogram('bias_histogram', bias)\n",
    "\n",
    "    conv = tf.nn.conv2d(data, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    linear_output = tf.nn.relu(tf.add(conv, bias))\n",
    "    pooling = tf.nn.max_pool(linear_output, ksize=pooling_size, strides=pooling_size, padding=\"SAME\")\n",
    "\n",
    "    return pooling\n",
    "\n",
    "def linear_layer(data, weights_size, biases_size):\n",
    "    weights = tf.get_variable(\"weights\", weights_size, initializer=tf.random_normal_initializer())\n",
    "    tf.summary.histogram('weights', weights)\n",
    "    biases = tf.get_variable(\"biases\", biases_size, initializer=tf.random_normal_initializer())\n",
    "    tf.summary.histogram('biases', biases)\n",
    "\n",
    "    return tf.add(tf.matmul(data, weights), biases)\n",
    "\n",
    "def convolutional_neural_network(data):\n",
    "    # 根据类别个数定义最后输出层的神经元\n",
    "    n_ouput_layer = 40\n",
    "\n",
    "    kernel_shape1=[5, 5, 1, 32]\n",
    "    kernel_shape2=[5, 5, 32, 64]\n",
    "    full_conn_w_shape = [15 * 12 * 64, 1024]\n",
    "    out_w_shape = [1024, n_ouput_layer]\n",
    "\n",
    "    bias_shape1=[32]\n",
    "    bias_shape2=[64]\n",
    "    full_conn_b_shape = [1024]\n",
    "    out_b_shape = [n_ouput_layer]\n",
    "\n",
    "    data = tf.reshape(data, [-1, 57, 47, 1])\n",
    "\n",
    "    # weights = {\n",
    "    #     \"w_conv1\": tf.get_variable(\"w_conv1\", kernel_shape1, initializer=tf.random_normal_initializer()),\n",
    "    #     \"w_conv2\": tf.get_variable(\"w_conv2\", kernel_shape2, initializer=tf.random_normal_initializer()),\n",
    "    #     \"w_fc\": tf.get_variable(\"w_fc\", [7 * 6 * 64, 1024], initializer=tf.random_normal_initializer()),\n",
    "    #     \"out\":  tf.get_variable(\"w_out\", [1024, n_ouput_layer], initializer=tf.random_normal_initializer())\n",
    "    # }\n",
    "\n",
    "    # biases = {\n",
    "    #     \"b_conv1\": tf.get_variable(\"b_conv1\", bias_shape1, initializer=tf.random_normal_initializer()),\n",
    "    #     \"b_conv2\": tf.get_variable(\"b_conv2\", bias_shape2, initializer=tf.random_normal_initializer()),\n",
    "    #     \"b_fc\": tf.get_variable(\"b_fc\", full_conn_b_shape, initializer=tf.random_normal_initializer()),\n",
    "    #     \"out\":  tf.get_variable(\"b_out\", [n_ouput_layer], initializer=tf.random_normal_initializer())\n",
    "    # }\n",
    "\n",
    "    # 经过第一层卷积神经网络后，得到的张量shape为：[batch, 19, 16, 32]\n",
    "    # conv1 = tf.nn.conv2d(data, weights['w_conv1'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    # ouput1 = tf.nn.relu(tf.add(conv1, biases['b_conv1']))\n",
    "    # layer1_output = tf.nn.max_pool(ouput1, ksize=[1, 3, 3, 1], strides=[1, 3, 3, 1], padding=\"SAME\")\n",
    "\n",
    "    # 经过第二层卷积神经网络后，得到的张量shape为：[batch, 7, 6, 64]\n",
    "    # conv2 = tf.nn.conv2d(layer1_output, weights['w_conv2'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    # ouput2 = tf.nn.relu(tf.add(conv2, biases['b_conv2']))\n",
    "    # layer2_output = tf.nn.max_pool(ouput2, ksize=[1, 3, 3, 1], strides=[1, 3, 3, 1], padding='SAME')\n",
    "\n",
    "    # 讲卷积层张量数据拉成2-D张量只有有一列的列向量\n",
    "    # layer2_output_flatten = tf.contrib.layers.flatten(layer2_output)\n",
    "    # full_conn_output = tf.nn.relu(tf.add(tf.matmul(layer2_output_flatten, weights[\"w_fc\"]), biases[\"b_fc\"]))\n",
    "\n",
    "    # output = tf.add(tf.matmul(layer3_output, weights[\"out\"]), biases[\"out\"])\n",
    "\n",
    "\n",
    "    # 经过第一层卷积神经网络后，得到的张量shape为：[batch, 29, 24, 32]\n",
    "    with tf.variable_scope(\"conv_layer1\") as layer1:\n",
    "        layer1_output = convolutional_layer(\n",
    "            data=data,\n",
    "            kernel_size=kernel_shape1,\n",
    "            bias_size=bias_shape1,\n",
    "            pooling_size=[1, 2, 2, 1]\n",
    "        )\n",
    "    # 经过第二层卷积神经网络后，得到的张量shape为：[batch, 15, 12, 64]\n",
    "    with tf.variable_scope(\"conv_layer2\") as layer2:\n",
    "        layer2_output = convolutional_layer(\n",
    "            data=layer1_output,\n",
    "            kernel_size=kernel_shape2,\n",
    "            bias_size=bias_shape2,\n",
    "            pooling_size=[1, 2, 2, 1]\n",
    "        )\n",
    "\n",
    "    with tf.variable_scope(\"full_connection\") as full_layer3:\n",
    "        # 讲卷积层张量数据拉成2-D张量只有有一列的列向量\n",
    "        layer2_output_flatten = tf.contrib.layers.flatten(layer2_output)\n",
    "        layer3_output = tf.nn.relu(\n",
    "            linear_layer(\n",
    "                data=layer2_output_flatten,\n",
    "                weights_size=full_conn_w_shape,\n",
    "                biases_size=full_conn_b_shape\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # layer3_output = tf.nn.dropout(layer3_output, 0.8)\n",
    "    with tf.variable_scope(\"output\") as output_layer4:\n",
    "        output = linear_layer(\n",
    "            data=layer3_output,\n",
    "            weights_size=out_w_shape,\n",
    "            biases_size=out_b_shape\n",
    "        )\n",
    "\n",
    "    return output;\n",
    "\n",
    "def plot(error_index, dataset_path):\n",
    "    img = mpimg.imread(dataset_path)\n",
    "    plt.imshow(img)\n",
    "    currentAxis = plt.gca()\n",
    "    for index in error_index:\n",
    "        row = index // 2\n",
    "        column = index % 2\n",
    "        currentAxis.add_patch(\n",
    "            patches.Rectangle(\n",
    "                xy=(\n",
    "                     47 * 9 if column == 0 else 47 * 19,\n",
    "                     row * 57\n",
    "                    ),\n",
    "                width=47,\n",
    "                height=57,\n",
    "                linewidth=1,\n",
    "                edgecolor='r',\n",
    "                facecolor='none'\n",
    "            )\n",
    "    )\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(11.40, 9.42)\n",
    "    plt.savefig(\"fig_result.png\", bbox_inches=\"tight\", dpi=100)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train():\n",
    "    batch_size = 40\n",
    "\n",
    "    dataset = load_data('olivettifaces.gif')\n",
    "    train_set_x, train_set_y = dataset[0]\n",
    "    vaild_set_x, valid_set_y = dataset[1]\n",
    "    test_set_x, test_set_y = dataset[2]\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [batch_size, 57 * 47])\n",
    "    Y = tf.placeholder(tf.float32, [batch_size, 40])\n",
    "\n",
    "    predict = convolutional_neural_network(X)\n",
    "    cost_func = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predict, labels=Y))\n",
    "    tf.summary.scalar(\"cost_func\", cost_func)\n",
    "    optimizer = tf.train.AdamOptimizer(1e-2).minimize(cost_func)\n",
    "\n",
    "    # 用于保存训练的最佳模型\n",
    "    saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\n",
    "    model_dir = './model'\n",
    "    model_path = model_dir + '/best.ckpt'\n",
    "\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        # 若不存在模型数据，需要训练模型参数\n",
    "        if not os.path.exists(model_path + \".index\"):\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            writer = tf.summary.FileWriter(\"./tmp/logs\", session.graph)\n",
    "\n",
    "            best_loss = float('Inf')\n",
    "\n",
    "            for epoch in range(20):\n",
    "                epoch_loss = 0\n",
    "                batch_num = np.shape(train_set_x)[0] // batch_size\n",
    "                for i in range(batch_num):\n",
    "                    step = epoch * batch_num + i\n",
    "                    x = train_set_x[i * batch_size: (i + 1) * batch_size]\n",
    "                    y = train_set_y[i * batch_size: (i + 1) * batch_size]\n",
    "\n",
    "                    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                    run_metadata = tf.RunMetadata()\n",
    "                    _, cost, summary = session.run([optimizer, cost_func, merged],\n",
    "                        feed_dict={X: x, Y: y},\n",
    "                        options=run_options,\n",
    "                        run_metadata=run_metadata)\n",
    "                    writer.add_run_metadata(run_metadata, 'step%d' % step)\n",
    "                    writer.add_summary(summary, step)\n",
    "                    epoch_loss += cost\n",
    "\n",
    "                print(epoch, ' : ', epoch_loss)\n",
    "                if best_loss >= epoch_loss:\n",
    "                    best_loss = epoch_loss\n",
    "                    if not os.path.exists(model_dir):\n",
    "                        os.mkdir(model_dir)\n",
    "                        print(\"create the directory: %s\" % model_dir)\n",
    "                    save_path = saver.save(session, model_path, write_meta_graph=False)\n",
    "                    print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "        # 恢复数据并校验和测试\n",
    "        saver.restore(session, model_path)\n",
    "        correct = tf.equal(tf.argmax(predict,1), tf.argmax(Y,1))\n",
    "        valid_accuracy = tf.reduce_mean(tf.cast(correct,'float'))\n",
    "        print('valid set accuracy: ', valid_accuracy.eval({X: vaild_set_x, Y: valid_set_y}))\n",
    "\n",
    "        test_pred = tf.argmax(predict, 1).eval({X: test_set_x})\n",
    "        test_true = np.argmax(test_set_y, 1)\n",
    "        test_correct = correct.eval({X: test_set_x, Y: test_set_y})\n",
    "        incorrect_index = [i for i in range(np.shape(test_correct)[0]) if not test_correct[i]]\n",
    "        for i in incorrect_index:\n",
    "            print('picture person is %i, but mis-predicted as person %i'\n",
    "                %(test_true[i], test_pred[i]))\n",
    "        plot(incorrect_index, \"olivettifaces.gif\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
