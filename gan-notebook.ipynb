{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "According to Yann LeCun, “adversarial training is the coolest thing since sliced bread.” Sliced bread certainly never created this much excitement within the deep learning community. Generative adversarial networks—or GANs, for short—have dramatically sharpened the possibility of AI-generated content, and have drawn active research efforts since they were [first described by Ian Goodfellow et al. in 2014](https://arxiv.org/abs/1406.2661).\n",
    "\n",
    "GANs are neural networks that learn to create synthetic data similar to some known input data. For instance, researchers have generated convincing images from [photographs of everything from bedrooms to album covers](https://github.com/Newmu/dcgan_code), and they display a remarkable ability to reflect [higher-order semantic logic](https://github.com/Newmu/dcgan_code).\n",
    "\n",
    "Those examples are fairly complex, but it's easy to build a GAN that generates very simple images. In this tutorial, we'll build a GAN that analyzes lots of images of handwritten digits and gradually learns to generate new images from scratch—*essentially, we'll be teaching a neural network how to write*.\n",
    "\n",
    "<img src=\"notebook-images/gan-animation.gif\" />\n",
    "_Sample images from the generative adversarial network that we'll build in this tutorial. During training, it gradually refines its ability to generate digits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN architecture\n",
    "\n",
    "Generative adversarial networks consist of two models: a generative model and a discriminative model.\n",
    "\n",
    "![caption](notebook-images/GAN_Overall.png)\n",
    "\n",
    "The discriminator model is a classifier that determines whether a given image looks like a real image from the dataset or like an artificially created image. This is basically a binary classifier that will take the form of a normal convolutional neural network (CNN).\n",
    "\n",
    "The generator model takes random input values and transforms them into images through a deconvolutional neural network.\n",
    "\n",
    "Over the course of many training iterations, the weights and biases in the discriminator and the generator are trained through backpropagation. The discriminator learns to tell \"real\" images of handwritten digits apart from \"fake\" images created by the generator. At the same time, the generator uses feedback from the discriminator to learn how to produce convincing images that the discriminator can't distinguish from real images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "We’re going to create a GAN that will generate handwritten digits that can fool even the best classifiers (and humans too, of course). We'll use [TensorFlow](https://www.tensorflow.org/), a deep learning library open-sourced by Google that makes it easy to train neural networks on GPUs.\n",
    "\n",
    "This tutorial expects that you're already at least a little bit familiar with TensorFlow. If you're not, we recommend reading \"[Hello, TensorFlow!](https://www.oreilly.com/learning/hello-tensorflow)\" or watching the \"[Hello, Tensorflow!](https://www.safaribooksonline.com/oriole/hello-tensorflow-oriole)\" interactive tutorial on Safari before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST variable we created above contains both the images and their labels, divided into a training set called `train` and a validation set called `validation`. (We won't need to worry about the labels in this tutorial.) We can retrieve batches of images by calling `next_batch` on `mnist`. Let's load one image and look at it.\n",
    "\n",
    "The images are initially formatted as a single row of 784 pixels. We can reshape them into 28 x 28-pixel images and view them using pyplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 784)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2c0b0671080>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIKh/ApaBFocl20dqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2c0b049c6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_image = mnist.train.next_batch(1)[0]\n",
    "print(sample_image.shape)\n",
    "\n",
    "sample_image = sample_image.reshape([28, 28])\n",
    "plt.imshow(sample_image, cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the cell above again, you'll see a different image from the MNIST training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator network\n",
    "\n",
    "Our discriminator is a convolutional neural network that takes in an image of size 28 x 28 x 1 as input and returns a single scalar number that describes whether or not the input image is \"real\" or \"fake\"—that is, whether it's drawn from the set of MNIST images or generated by the generator.\n",
    "\n",
    "![caption](notebook-images/GAN_Discriminator.png)\n",
    "\n",
    "The structure of our discriminator network is based closely on [TensorFlow's sample CNN classifier model](https://www.tensorflow.org/get_started/mnist/pros). It features two convolutional layers that find 5x5-pixel features, and two \"fully connected\" layers that multiply weights by every pixel in the image.\n",
    "\n",
    "To set up each layer, we start by creating weight and bias variables through [`tf.get_variable`](https://www.tensorflow.org/api_docs/python/tf/get_variable). Weights are initialized from a [truncated normal](https://www.tensorflow.org/api_docs/python/tf/truncated_normal) distribution, and biases are initialized at zero.\n",
    "\n",
    "[`tf.nn.conv2d()`](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d) is TensorFlow's standard convolution function. It takes 4 arguments. The first is the input volume (our `28 x 28 x 1` images in this case). The next argument is the filter/weight matrix. Finally, you can also change the stride and padding of the convolution. Those two values affect the dimensions of the output volume.\n",
    "\n",
    "If you're already comfortable with CNNs, you'll recognize this as a simple binary classifier—nothing fancy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(images, reuse_variables=None):\n",
    "    with tf.variable_scope(tf.get_variable_scope(), reuse=reuse_variables) as scope:\n",
    "        # First convolutional and pool layers\n",
    "        # This finds 32 different 5 x 5 pixel features\n",
    "        d_w1 = tf.get_variable('d_w1', [5, 5, 1, 32], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        d_b1 = tf.get_variable('d_b1', [32], initializer=tf.constant_initializer(0))\n",
    "        d1 = tf.nn.conv2d(input=images, filter=d_w1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        d1 = d1 + d_b1\n",
    "        d1 = tf.nn.relu(d1)\n",
    "        d1 = tf.nn.avg_pool(d1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "        # Second convolutional and pool layers\n",
    "        # This finds 64 different 5 x 5 pixel features\n",
    "        d_w2 = tf.get_variable('d_w2', [5, 5, 32, 64], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        d_b2 = tf.get_variable('d_b2', [64], initializer=tf.constant_initializer(0))\n",
    "        d2 = tf.nn.conv2d(input=d1, filter=d_w2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        d2 = d2 + d_b2\n",
    "        d2 = tf.nn.relu(d2)\n",
    "        d2 = tf.nn.avg_pool(d2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "        # First fully connected layer\n",
    "        d_w3 = tf.get_variable('d_w3', [7 * 7 * 64, 1024], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        d_b3 = tf.get_variable('d_b3', [1024], initializer=tf.constant_initializer(0))\n",
    "        d3 = tf.reshape(d2, [-1, 7 * 7 * 64])\n",
    "        d3 = tf.matmul(d3, d_w3)\n",
    "        d3 = d3 + d_b3\n",
    "        d3 = tf.nn.relu(d3)\n",
    "\n",
    "        # Second fully connected layer\n",
    "        d_w4 = tf.get_variable('d_w4', [1024, 1], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        d_b4 = tf.get_variable('d_b4', [1], initializer=tf.constant_initializer(0))\n",
    "        d4 = tf.matmul(d3, d_w4) + d_b4\n",
    "\n",
    "        # d4 contains unscaled values\n",
    "        return d4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator network\n",
    "\n",
    "![caption](notebook-images/GAN_Generator.png)\n",
    "\n",
    "Now that we have our discriminator defined, let’s take a look at the generator model. We'll base the overall structure of our model on a simple generator published by [Tim O'Shea](https://github.com/osh/KerasGAN).\n",
    "\n",
    "You can think of the generator as a kind of reverse convolutional neural network. A typical CNN like our discriminator network transforms a 2- or 3-dimensional matrix of pixel values into a single probability. A generator, however, takes a `d`-dimensional vector of noise and upsamples it to become a 28 x 28 image. ReLU and batch normalization are used to stabilize the outputs of each layer.\n",
    "\n",
    "In our generator network, we use three convolutional layers along with interpolation until a `28 x 28` pixel image is formed. (Actually, as you'll see below, we've taken care to form `28 x 28 x 1` images; many TensorFlow tools for dealing with images anticipate that the images will have some number of _channels_—usually 1 for greyscale images or 3 for RGB color images.)\n",
    "\n",
    "At the output layer we add a [`tf.sigmoid()`](https://www.tensorflow.org/api_docs/python/tf/sigmoid) activation function; this squeezes pixels that would appear grey toward either black or white, resulting in a crisper image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z, batch_size, z_dim):\n",
    "    g_w1 = tf.get_variable('g_w1', [z_dim, 3136], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    g_b1 = tf.get_variable('g_b1', [3136], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    g1 = tf.matmul(z, g_w1) + g_b1\n",
    "    g1 = tf.reshape(g1, [-1, 56, 56, 1])\n",
    "    g1 = tf.contrib.layers.batch_norm(g1, epsilon=1e-5, scope='g_b1')\n",
    "    g1 = tf.nn.relu(g1)\n",
    "\n",
    "    # Generate 50 features\n",
    "    g_w2 = tf.get_variable('g_w2', [3, 3, 1, z_dim/2], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    g_b2 = tf.get_variable('g_b2', [z_dim/2], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    g2 = tf.nn.conv2d(g1, g_w2, strides=[1, 2, 2, 1], padding='SAME')\n",
    "    g2 = g2 + g_b2\n",
    "    g2 = tf.contrib.layers.batch_norm(g2, epsilon=1e-5, scope='g_b2')\n",
    "    g2 = tf.nn.relu(g2)\n",
    "    g2 = tf.image.resize_images(g2, [56, 56])\n",
    "\n",
    "    # Generate 25 features\n",
    "    g_w3 = tf.get_variable('g_w3', [3, 3, z_dim/2, z_dim/4], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    g_b3 = tf.get_variable('g_b3', [z_dim/4], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    g3 = tf.nn.conv2d(g2, g_w3, strides=[1, 2, 2, 1], padding='SAME')\n",
    "    g3 = g3 + g_b3\n",
    "    g3 = tf.contrib.layers.batch_norm(g3, epsilon=1e-5, scope='g_b3')\n",
    "    g3 = tf.nn.relu(g3)\n",
    "    g3 = tf.image.resize_images(g3, [56, 56])\n",
    "\n",
    "    # Final convolution with one output channel\n",
    "    g_w4 = tf.get_variable('g_w4', [1, 1, z_dim/4, 1], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    g_b4 = tf.get_variable('g_b4', [1], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    g4 = tf.nn.conv2d(g3, g_w4, strides=[1, 2, 2, 1], padding='SAME')\n",
    "    g4 = g4 + g_b4\n",
    "    g4 = tf.sigmoid(g4)\n",
    "    \n",
    "    # Dimensions of g4: batch_size x 28 x 28 x 1\n",
    "    return g4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a sample image\n",
    "\n",
    "Now we’ve defined both the generator and discriminator functions. Let’s see what a sample output from an untrained generator looks like.\n",
    "\n",
    "We need to open a TensorFlow session and create a placeholder for the input to our generator. The shape of the placeholder will be `None, z_dimensions`. The `None` keyword means that the value can be determined at session runtime. We normally have `None` as our first dimension so that we can have variable batch sizes. (With a batch size of 50, the input to the generator would be 50 x 100). With the `None` keywoard, we don't have to specify `batch_size` until later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z_dimensions = 100\n",
    "z_placeholder = tf.placeholder(tf.float32, [None, z_dimensions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a variable (`generated_image_output`) that holds the output of the generator, and we'll also initialize the random noise vector that we're going to use as input. The [`np.random.normal()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.normal.html) function has three arguments. The first and second define the mean and standard deviation for the normal distribution (0 and 1 in our case), and the third defines the the shape of the vector (`1 x 100`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generated_image_output = generator(z_placeholder, 1, z_dimensions)\n",
    "z_batch = np.random.normal(0, 1, [1, z_dimensions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize all the variables, feed our `z_batch` into the placeholder, and run the session.\n",
    "\n",
    "The [`sess.run()`](https://www.tensorflow.org/api_docs/python/tf/Session#run) function has two arguments. The first is called the \"fetches\" argument; it defines the value you're interested in computing. In our case, we want to see what the output of the generator is. If you look back at the last code snippet, you'll see that the output of the generator function is stored in `generated_image_output`, so we'll use `generated_image_output` for our first argument.\n",
    "\n",
    "The second argument takes a dictionary of inputs that are substituted into the graph when it runs. This is where we feed in our placeholders. In our example, we need to feed our `z_batch` variable into the `z_placeholder` that we defined earlier. As before, we'll view the image by reshaping it to `28 x 28` pixels and show it with PyPlot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUvGLZOr/ALXdod1tuD9AAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2c0b09ee278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    generated_image = sess.run(generated_image_output,\n",
    "                                feed_dict={z_placeholder: z_batch})\n",
    "    generated_image = generated_image.reshape([28, 28])\n",
    "    plt.imshow(generated_image, cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks like noise, right? Now we need to train the weights and biases in the generator network to convert random numbers into recognizable digits. Let's look at loss functions and optimization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a GAN\n",
    "\n",
    "One of the trickiest parts about building and tuning GANs is that they have two loss functions: one that encourages the generator to create better images, and the other that encourages the discriminator to distinguish generated images from real images.\n",
    "\n",
    "We train both the generator and the discriminator simultaneously. As the discriminator gets better at distinguishing real images from generated images, the generator is able to better tune its weights and biases to generate convincing images.\n",
    "\n",
    "Here are the inputs and outputs for our networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_size = 50\n",
    "\n",
    "z_placeholder = tf.placeholder(tf.float32, [None, z_dimensions], name='z_placeholder') \n",
    "# z_placeholder is for feeding input noise to the generator\n",
    "\n",
    "x_placeholder = tf.placeholder(tf.float32, shape = [None,28,28,1], name='x_placeholder') \n",
    "# x_placeholder is for feeding input images to the discriminator\n",
    "\n",
    "Gz = generator(z_placeholder, batch_size, z_dimensions) \n",
    "# Gz holds the generated images\n",
    "\n",
    "Dx = discriminator(x_placeholder) \n",
    "# Dx will hold discriminator prediction probabilities\n",
    "# for the real MNIST images\n",
    "\n",
    "Dg = discriminator(Gz, reuse_variables=True)\n",
    "# Dg will hold discriminator prediction probabilities for generated images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let’s first think about what we want out of our networks. The discriminator's goal is to correctly label real MNIST images as real (return a higher output) and generated images as fake (return a lower output). We'll calculate two losses for the discriminator: one loss that compares `Dx` and 1 for real images from the MNIST set, as well as a loss that compares `Dg` and 0 for images from the generator. We'll do this with TensorFlow's [`tf.nn.sigmoid_cross_entropy_with_logits()`](https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits) function, which calculates the cross-entropy losses between `Dx` and 1 and between `Dg` and 0.\n",
    "\n",
    "`sigmoid_cross_entropy_with_logits` operates on unscaled values rather than probability values from 0 to 1. Take a look at the last line of our discriminator: there's no softmax or sigmoid layer at the end. GANs can fail if their discriminators \"saturate,\" or become confident enough to return exactly 0 when they're given a generated image; that leaves the discriminator without a useful gradient to descend.\n",
    "\n",
    "The [`tf.reduce_mean()`](https://www.tensorflow.org/api_docs/python/tf/reduce_mean) function takes the mean value of all of the components in the matrix returned by the cross entropy function. This is a way of reducing the loss to a single scalar value, instead of a vector or matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = Dx, labels = tf.ones_like(Dx)))\n",
    "d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = Dg, labels = tf.zeros_like(Dg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set up the generator's loss function. We want the generator network to create images that will fool the discriminator: the generator wants the discriminator to output a value close to 1 when it's given an image from the generator. Therefore, we want to compute the loss between `Dg` and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = Dg, labels = tf.ones_like(Dg)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our loss functions, we need to define our optimizers. The optimizer for the generator network needs to only update the generator’s weights, not those of the discriminator. Likewise, when we train the discriminator, we want to hold the generator's weights fixed.\n",
    "\n",
    "In order to make this distinction, we need to create two lists of variables, one with the discriminator’s weights and biases and another with the generator’s weights and biases. This is where naming all of your TensorFlow variables with a thoughtful scheme can come in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d_w1:0', 'd_b1:0', 'd_w2:0', 'd_b2:0', 'd_w3:0', 'd_b3:0', 'd_w4:0', 'd_b4:0']\n",
      "['g_w1:0', 'g_b1:0', 'g_b1/beta:0', 'g_w2:0', 'g_b2:0', 'g_b2/beta:0', 'g_w3:0', 'g_b3:0', 'g_b3/beta:0', 'g_w4:0', 'g_b4:0']\n"
     ]
    }
   ],
   "source": [
    "tvars = tf.trainable_variables()\n",
    "\n",
    "d_vars = [var for var in tvars if 'd_' in var.name]\n",
    "g_vars = [var for var in tvars if 'g_' in var.name]\n",
    "\n",
    "print([v.name for v in d_vars])\n",
    "print([v.name for v in g_vars])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we specify our two optimizers. [Adam](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) is usually the optimization algorithm of choice for GANs; it utilizes adaptive learning rates and momentum. We call Adam's minimize function and also specify the variables that we want it to update—the generator's weights and biases when we train the generator, and the discriminator's weights and biases when we train the discriminator.\n",
    "\n",
    "We're setting up two different training operations for the discriminator here: one that trains the discriminator on real images and one that trains the discrmnator on fake images. It's sometimes useful to use different learning rates for these two training operations, or to use them separately to [regulate learning in other ways](https://github.com/jonbruner/ezgan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the discriminator\n",
    "d_trainer_fake = tf.train.AdamOptimizer(0.0003).minimize(d_loss_fake, var_list=d_vars)\n",
    "d_trainer_real = tf.train.AdamOptimizer(0.0003).minimize(d_loss_real, var_list=d_vars)\n",
    "\n",
    "# Train the generator\n",
    "g_trainer = tf.train.AdamOptimizer(0.0001).minimize(g_loss, var_list=g_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be tricky to get GANs to converge, and moreover they often need to train for a very long time. [TensorBoard](https://www.tensorflow.org/how_tos/summaries_and_tensorboard/) is useful for tracking the training process; it can graph scalar properties like losses, display sample images during training, and illustrate the topology of the neural networks.\n",
    "\n",
    "If you run this script on your own machine, include the cell below. Then, in a terminal window from the directory that this notebook lives in, run\n",
    "\n",
    "```\n",
    "tensorboard --logdir=tensorboard/\n",
    "```\n",
    "\n",
    "and open TensorBoard by visiting [`http://localhost:6006`](http://localhost:6006) in your web browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From this point forward, reuse variables\n",
    "tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "tf.summary.scalar('Generator_loss', g_loss)\n",
    "tf.summary.scalar('Discriminator_loss_real', d_loss_real)\n",
    "tf.summary.scalar('Discriminator_loss_fake', d_loss_fake)\n",
    "\n",
    "images_for_tensorboard = generator(z_placeholder, batch_size, z_dimensions)\n",
    "tf.summary.image('Generated_images', images_for_tensorboard, 5)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we iterate. We begin by briefly giving the discriminator some initial training; this helps it develop a gradient that's useful to the generator.\n",
    "\n",
    "Then we move on to the main training loop. When we train the generator, we’ll feed a random `z` vector into the generator and pass its output to the discriminator (this is the `Dg` variable we specified earlier). The generator’s weights and biases will be updated in order to produce images that the discriminator is more likely to classify as real.\n",
    "\n",
    "To train the discriminator, we’ll feed it a batch of images from the MNIST set to serve as the positive examples, and then train the discriminator again on generated images, using them as negative examples. Remember that as the generator improves its output, the discriminator continues to learn to classify the improved generator images as fake.\n",
    "\n",
    "Because it takes a long time to train a GAN, **we recommend not running this code block if you're going through this tutorial for the first time**. Instead, follow along but then run the following code block, which loads a pre-trained model for us to continue the tutorial.\n",
    "\n",
    "**If you want to run this code yourself, prepare to wait: it takes about three hours on a fast GPU, but could take ten times that long on a desktop CPU.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLossReal: 0.693015 dLossFake: 0.710096\n",
      "dLossReal: 0.00354401 dLossFake: 0.0180629\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-159a81e2095d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mreal_image_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     _, __, dLossReal, dLossFake = sess.run([d_trainer_real, d_trainer_fake, d_loss_real, d_loss_fake],\n\u001b[1;32m----> 9\u001b[1;33m                                            {x_placeholder: real_image_batch, z_placeholder: z_batch})\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Pre-train discriminator\n",
    "for i in range(300):\n",
    "    z_batch = np.random.normal(0, 1, size=[batch_size, z_dimensions])\n",
    "    real_image_batch = mnist.train.next_batch(batch_size)[0].reshape([batch_size, 28, 28, 1])\n",
    "    _, __, dLossReal, dLossFake = sess.run([d_trainer_real, d_trainer_fake, d_loss_real, d_loss_fake],\n",
    "                                           {x_placeholder: real_image_batch, z_placeholder: z_batch})\n",
    "\n",
    "    if(i % 100 == 0):\n",
    "        print(\"dLossReal:\", dLossReal, \"dLossFake:\", dLossFake)\n",
    "\n",
    "# Train generator and discriminator together\n",
    "for i in range(100000):\n",
    "    real_image_batch = mnist.train.next_batch(batch_size)[0].reshape([batch_size, 28, 28, 1])\n",
    "    z_batch = np.random.normal(0, 1, size=[batch_size, z_dimensions])\n",
    "\n",
    "    # Train discriminator on both real and fake images\n",
    "    _, __, dLossReal, dLossFake = sess.run([d_trainer_real, d_trainer_fake, d_loss_real, d_loss_fake],\n",
    "                                           {x_placeholder: real_image_batch, z_placeholder: z_batch})\n",
    "\n",
    "    # Train generator\n",
    "    z_batch = np.random.normal(0, 1, size=[batch_size, z_dimensions])\n",
    "    _ = sess.run(g_trainer, feed_dict={z_placeholder: z_batch})\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        # Update TensorBoard with summary statistics\n",
    "        z_batch = np.random.normal(0, 1, size=[batch_size, z_dimensions])\n",
    "        summary = sess.run(merged, {z_placeholder: z_batch, x_placeholder: real_image_batch})\n",
    "        writer.add_summary(summary, i)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        # Every 100 iterations, show a generated image\n",
    "        print(\"Iteration:\", i, \"at\", datetime.datetime.now())\n",
    "        z_batch = np.random.normal(0, 1, size=[1, z_dimensions])\n",
    "        generated_images = generator(z_placeholder, 1, z_dimensions)\n",
    "        images = sess.run(generated_images, {z_placeholder: z_batch})\n",
    "        plt.imshow(images[0].reshape([28, 28]), cmap='Greys')\n",
    "        plt.show()\n",
    "\n",
    "        # Show discriminator's estimate\n",
    "        im = images[0].reshape([1, 28, 28, 1])\n",
    "        result = discriminator(x_placeholder)\n",
    "        estimate = sess.run(result, {x_placeholder: im})\n",
    "        print(\"Estimate:\", estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it can take so long to train a GAN, we recommend that you skip the cell above and execute the following cell. It loads a model that we've already trained for several hours on a fast GPU machine, and lets you experiment with the output of a trained GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from pretrained-model/pretrained_gan.ckpt\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to get matching files on pretrained-model/pretrained_gan.ckpt: Not found: FindFirstFile failed for: pretrained-model : The system cannot find the path specified.\r\n\n\t [[Node: save_1/RestoreV2_84 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save_1/Const_0_0, save_1/RestoreV2_84/tensor_names, save_1/RestoreV2_84/shape_and_slices)]]\n\nCaused by op 'save_1/RestoreV2_84', defined at:\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-17-abe1e34dd90e>\", line 1, in <module>\n    saver = tf.train.Saver()\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1139, in __init__\n    self.build()\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1170, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 691, in build\n    restore_sequentially, reshape)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 407, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 247, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 640, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to get matching files on pretrained-model/pretrained_gan.ckpt: Not found: FindFirstFile failed for: pretrained-model : The system cannot find the path specified.\r\n\n\t [[Node: save_1/RestoreV2_84 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save_1/Const_0_0, save_1/RestoreV2_84/tensor_names, save_1/RestoreV2_84/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to get matching files on pretrained-model/pretrained_gan.ckpt: Not found: FindFirstFile failed for: pretrained-model : The system cannot find the path specified.\r\n\n\t [[Node: save_1/RestoreV2_84 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save_1/Const_0_0, save_1/RestoreV2_84/tensor_names, save_1/RestoreV2_84/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-abe1e34dd90e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'pretrained-model/pretrained_gan.ckpt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mz_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_dimensions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mz_placeholder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_dimensions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'z_placeholder'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1546\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Restoring parameters from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1547\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[1;32m-> 1548\u001b[1;33m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[0;32m   1549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to get matching files on pretrained-model/pretrained_gan.ckpt: Not found: FindFirstFile failed for: pretrained-model : The system cannot find the path specified.\r\n\n\t [[Node: save_1/RestoreV2_84 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save_1/Const_0_0, save_1/RestoreV2_84/tensor_names, save_1/RestoreV2_84/shape_and_slices)]]\n\nCaused by op 'save_1/RestoreV2_84', defined at:\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-17-abe1e34dd90e>\", line 1, in <module>\n    saver = tf.train.Saver()\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1139, in __init__\n    self.build()\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1170, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 691, in build\n    restore_sequentially, reshape)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 407, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 247, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 640, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\ZHANG Wei\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to get matching files on pretrained-model/pretrained_gan.ckpt: Not found: FindFirstFile failed for: pretrained-model : The system cannot find the path specified.\r\n\n\t [[Node: save_1/RestoreV2_84 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save_1/Const_0_0, save_1/RestoreV2_84/tensor_names, save_1/RestoreV2_84/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, 'pretrained-model/pretrained_gan.ckpt')\n",
    "    z_batch = np.random.normal(0, 1, size=[10, z_dimensions])\n",
    "    z_placeholder = tf.placeholder(tf.float32, [None, z_dimensions], name='z_placeholder') \n",
    "    generated_images = generator(z_placeholder, 10, z_dimensions)\n",
    "    images = sess.run(generated_images, {z_placeholder: z_batch})\n",
    "    for i in range(10):\n",
    "        plt.imshow(images[i].reshape([28, 28]), cmap='Greys')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training difficulties\n",
    "\n",
    "GANs are notoriously difficult to train. Without the right hyperparameters, network architecture, and training procedure, the discriminator can overpower the generator, or vice-versa.\n",
    "\n",
    "In one common failure mode, the discriminator overpowers the generator, classifying generated images as fake with absolute certainty. When the discriminator responds with absolute certainty, it leaves no gradient for the generator to descend. This is partly why we built our discriminator to produce unscaled output rather than passing its output through a sigmoid function that would push its evaluation toward either 0 or 1.\n",
    "\n",
    "In another common failure mode known as **mode collapse**, the generator discovers and exploits some weakness in the discriminator. You can recognize mode collapse in your GAN if it generates many very similar images regardless of variation in the generator input _z_. Mode collapse can sometimes be corrected by \"strengthening\" the discriminator in some way—for instance, by adjusting its training rate or by reconfiguring its layers.\n",
    "\n",
    "Researchers have identified a handful of [\"GAN hacks\"](https://github.com/soumith/ganhacks) that can be helpful in building stable GANs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing thoughts\n",
    "\n",
    "GANs have tremendous potential to reshape the digital world that we interact with every day. The field is still very young, and the next great GAN discovery could be yours!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other resources\n",
    "\n",
    "- [The original GAN paper](https://arxiv.org/abs/1406.2661) by Ian Goodfellow and his collaborators, published in 2014\n",
    "- [A more recent tutorial by Goodfellow](https://arxiv.org/abs/1701.00160) that explains GANs in somewhat more accessible terms\n",
    "- [A paper by Alec Radford, Luke Metz, and Soumith Chintala](https://arxiv.org/abs/1511.06434) that introduces deep convolutional GANs, whose basic structure we use in our generator in this tutorial. Also see [their DCGAN code on GitHub](https://github.com/Newmu/dcgan_code).\n",
    "- [A reference collection of generative networks by Agustinus Kristiadi](https://github.com/wiseodd/generative-models), implemented in TensorFlow"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
